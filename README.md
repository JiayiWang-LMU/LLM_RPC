# LLM RPC

This project helps users compare the quality of expected results from Large Language Models (LLMs) using only a prompt. By running the `generate_ranking.py` script and inputting a prompt, the system will output a ranking of LLMs that provide the best answers according to the **MT-bench dataset**, which is based on human experts' judgments.

## How It Works
1. **Run the Script**: Execute the `generate_ranking.py` script.
2. **Input a Prompt**: Provide a prompt when prompted by the system.
3. **Get Rankings**: The system will output a ranking of LLMs based on the quality of their responses.
